slurm_config: big
task_type: local_predict
dataset:
  split: test
  video_processor: ShardedVideoProcessor
  aligner: FixedLenAligner
  bert_name: bert-base-uncased
  meta_processor: ShardedHow2MetaProcessor
  test_path: data/feat/my_videos_val.lst
  trainval_annotation: data/my_videos/raw_caption.json
  use_annotation_text: true
  vfeat_dir: data/feat/feat_my_videos_s3d_shard_small
  tfeat_dir: data/feat/feat_my_videos_s3d_shard_small/raw_caption_dedup.bert-base-uncased.
  text_processor: ShardedTextProcessor
  subsampling: 1
  sampled_min_len: 8
  sampled_max_len: 64
  max_video_len: 32
  max_len: 64
  lazy_vfeat_mask: true
  mfm_probability: 0.15
  mlm_probability: 0.15
  mm_prob: 0.5
  sampled_video_min_len: 3
  sampled_video_max_len: 32
  num_video_per_batch: 32
  clip_per_video: 16
fairseq:
  dataset:
    batch_size: 16
    valid_subset: test
    num_workers: 1
  common_eval:
    path: runs/retri/videoclip/checkpoint_best.pt
model:
  model_cls: MMFusionSeparate
  mm_encoder_cls: null
  video_encoder_cls: MMBertForEncoder
  text_encoder_cls: BertModel
  num_hidden_video_layers: 6
eval:
  save_path: runs/retri/videoclip/my_videos_zs/eval
metric: RetrievalMetric
predictor: RetrievalPredictor
